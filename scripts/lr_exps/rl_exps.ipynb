{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- implement an agent to switch learning rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tidianes/miniconda3/envs/tid_env/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# from ray import tune\n",
    "\n",
    "import stable_baselines3\n",
    "from stable_baselines3.common import vec_env, monitor\n",
    "\n",
    "\n",
    "from rl_utils import lr_Environment, make_observation\n",
    "from lr_utils import minimize_custom, norm_function, rosen_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_dim = 1 # dimension of the problem\n",
    "objective_function = norm_function\n",
    "x_0 = 0.0#torch.tensor([0.0], requires_grad=True)\n",
    "optimizer_class = torch.optim.SGD #torch.optim.Adam,\n",
    "precision = 1e-6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.019999999552965164, 0.03959999978542328, 0.05880799889564514],\n",
       " [0.9604000008761883, 0.922368160412159, 0.8858423829428199])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minimize_custom(objective=objective_function,\n",
    "                optimizer_class=optimizer_class,\n",
    "                x_0=x_0,#0.,#[1.3, 0.7, 0.8, 1.9, 1.2],\n",
    "                lr=1e-2,\n",
    "                steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.07844541073241888\n"
     ]
    }
   ],
   "source": [
    "norm_dataset = [norm_function for _ in range(90)]\n",
    "norm_env = lr_Environment(norm_dataset, num_steps=40, history_len=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.7131484739612176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tidianes/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/common/env_checker.py:130: UserWarning: Your observation  has an unconventional shape (neither an image, nor a 1D vector). We recommend you to flatten the observation to have only a 1D vector or use a custom policy to properly process the data.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "check_env(norm_env, warn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tidianes/miniconda3/envs/tid_env/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py:151: UserWarning: You have specified a mini-batch size of 64, but because the `RolloutBuffer` is of size `n_steps * n_envs = 2`, after every 0 untruncated mini-batches, there will be a truncated mini-batch of size 2\n",
      "We recommend using a `batch_size` that is a factor of `n_steps * n_envs`.\n",
      "Info: (n_steps=2 and n_envs=1)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "norm_env = vec_env.DummyVecEnv([\n",
    "    lambda: monitor.Monitor(\n",
    "        lr_Environment(norm_function, num_steps=40, history_len=25)\n",
    "    )\n",
    "]*32)\n",
    "\"\"\"\n",
    "\n",
    "norm_policy = stable_baselines3.PPO('MlpPolicy', norm_env, n_steps=2, verbose=0,\n",
    "                                         tensorboard_log='tb_logs/norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.37193696013476235\n",
      "0.6282094179192989\n",
      "-0.12306061526243421\n",
      "0.12822276013454356\n",
      "-1.3962438834559574\n",
      "-1.3788632605374942\n",
      "-0.2848139659451785\n",
      "1.4634918168198348\n",
      "1.3337478870933546\n",
      "0.18357046192797652\n",
      "-2.668042346939855\n",
      "-0.14449272427753254\n",
      "1.4113482250793854\n",
      "0.3081771331219661\n",
      "-1.1555765373475988\n",
      "-0.42454029445311303\n",
      "0.43245584571249307\n",
      "1.2558896520628196\n",
      "-1.003950188442902\n",
      "-0.13332534745717248\n",
      "-0.7744835498382064\n",
      "-0.36442148859335965\n",
      "-1.4081922421952433\n",
      "-1.1357559692419805\n",
      "1.2077401486295967\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x7f5292a04640>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_timesteps = 1000 #20*40*90\n",
    "\n",
    "#quadratic_policy.learn(total_timesteps=20 * quadratic_env.envs[0].num_steps * len(norm_dataset))\n",
    "norm_policy.learn(total_timesteps=total_timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.171675090980155\n",
      "Step 1\n",
      "Action:  3\n",
      "reward= -3.823715925216675\n",
      "Step 2\n",
      "Action:  3\n",
      "reward= -3.100141763687134\n",
      "Step 3\n",
      "Action:  3\n",
      "reward= -2.5134918689727783\n",
      "Step 4\n",
      "Action:  3\n",
      "reward= -2.037855625152588\n",
      "Step 5\n",
      "Action:  3\n",
      "reward= -1.652225375175476\n",
      "Step 6\n",
      "Action:  3\n",
      "reward= -1.3395695686340332\n",
      "Step 7\n",
      "Action:  3\n",
      "reward= -1.086078405380249\n",
      "Step 8\n",
      "Action:  3\n",
      "reward= -0.8805561065673828\n",
      "Step 9\n",
      "Action:  3\n",
      "reward= -0.7139257192611694\n",
      "Step 10\n",
      "Action:  3\n",
      "reward= -0.5788271427154541\n",
      "Step 11\n",
      "Action:  3\n",
      "reward= -0.46929383277893066\n",
      "Step 12\n",
      "Action:  3\n",
      "reward= -0.3804878294467926\n",
      "Step 13\n",
      "Action:  3\n",
      "reward= -0.3084869086742401\n",
      "Step 14\n",
      "Action:  3\n",
      "reward= -0.2501108646392822\n",
      "Step 15\n",
      "Action:  3\n",
      "reward= -0.20278161764144897\n",
      "Step 16\n",
      "Action:  3\n",
      "reward= -0.1644085943698883\n",
      "Step 17\n",
      "Action:  3\n",
      "reward= -0.13329702615737915\n",
      "Step 18\n",
      "Action:  3\n",
      "reward= -0.10807280987501144\n",
      "Step 19\n",
      "Action:  3\n",
      "reward= -0.08762182295322418\n",
      "Step 20\n",
      "Action:  3\n",
      "reward= -0.07104086875915527\n"
     ]
    }
   ],
   "source": [
    "# Test the trained agent\n",
    "actions = []\n",
    "obs = norm_env.reset()\n",
    "n_steps = 20\n",
    "for step in range(n_steps):\n",
    "  action, _ = norm_policy.predict(obs, deterministic=True)\n",
    "  actions.append(action)\n",
    "  print(\"Step {}\".format(step + 1))\n",
    "  print(\"Action: \", action)\n",
    "  obs, reward, done, info = norm_env.step(action)\n",
    "  #print('obs=', obs, 'reward=', reward, 'done=', done)\n",
    "  print('reward=', reward)\n",
    "  #norm_env.render(mode='console')\n",
    "  if done:\n",
    "    # Note that the VecEnv resets automatically\n",
    "    # when a done signal is encountered\n",
    "    print(\"Goal reached!\", \"reward=\", reward)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('tid_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e4ac7f30aa65cc641f5967c96ff1707a40f829f0dd175c4897139c8c0fb2d9eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
