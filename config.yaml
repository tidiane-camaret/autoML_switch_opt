model:
  num_problems : 4500
  num_agent_runs : 4500
  model_training_steps : 700
  history_len: 50
  untrained_agent_episode_steps : 10
  lr : 0.01
  
problem : 'AckleyProblem' #options:['MNIST', 'SquareProblem', 'AckleyProblem', 'MLPProblem']

policy:
  model: 'PPO' # options:['DQN', 'PPO']  info: DQN for hard method, PPO for soft method.
  exploration_fraction : 0.5
  optimization_mode : 'soft' # options:['soft', 'hard']

environment:
  reward_system : "function" # options:['function', 'lookahead']  # info: Lookahead only for hard method.
  optimizer_storing : "dict" # options:['dict', optimizer_class']