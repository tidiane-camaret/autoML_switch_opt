model:
  num_problems : 1000
  num_agent_runs : 1000
  model_training_steps : 250
  history_len: 25
  epochs : 10
  lr : 0.01

  
problem : 'MNIST' #options:['MNIST', 'SquareProblem', 'AckleyProblem', 'MLPProblem']


policy:
  model: 'PPO' # options:['DQN', 'PPO']  info: DQN for hard method, PPO for soft method.
  exploration_fraction : 0.5
  optimization_mode : 'soft' # options:['soft', 'hard']

environment:
  reward_system : "function" # options:['function', 'lookahead']  # info: Lookahead only for hard method.
  optimizer_storing : "dict" # options:['dict', optimizer_class']